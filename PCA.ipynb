{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PCA\n",
    "\n",
    "$\\bf Side \\ Story :$ Can you use as little as possible features to represents the variance of samples ?\n",
    "\n",
    "$\\bf Idea :$ Choose the top k greatest eigenvals corrseponding eigenvectors from covariance matrix\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "X_{n \\times p} = \\begin{bmatrix}\n",
    "\\vdots &  & \\vdots \\\\\n",
    "x_1 & \\cdots & x_n \\\\\n",
    "\\vdots &  & \\vdots \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "be the matrix whose rows are samples and columns are features.\n",
    "\n",
    "We hope to remove some unimportant features for dimension reduction and try to keep the spatially information from higher dimension to lower dimension.\n",
    "\n",
    "Note that the spatially information, or variance, can be stored as the covariance matrix, which have the form below:\n",
    "\n",
    "$$\n",
    "C_{p\\times p} = \\frac{1}{n}X^\\top X = \\frac{1}{n}\\begin{bmatrix}\n",
    "\\left< x_1, x_1 \\right> & \\left< x_1, x_2 \\right> & \\cdots & \\left< x_1, x_p \\right> \\\\\n",
    "\\left< x_2, x_1 \\right> & \\left< x_2, x_2 \\right> & \\cdots & \\left< x_2, x_p \\right> \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\left< x_p, x_1 \\right> & \\left< x_n, x_2 \\right> & \\cdots & \\left< x_p, x_p \\right>\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- $C$ is the positive semidefinite matrix, which means that all the eigenvalues of $C$ greater or equal than $0$\n",
    "  - for any $z \\in \\mathbb{R}^p$, $z^\\top Cz = \\frac{1}{n}(Xz)^\\top (Xz) = \\left\\| Xz\\right\\| \\geq 0$\n",
    "- Note that $\\frac{1}{n}X^\\top X$ is the covariance matrix of features but $\\frac{1}{n}XX^\\top$ is the covariance matrix of samples\n",
    "\n",
    "After we get the covariance matrix of features $C$, we apply EVD on $C$ to obtain the top k important eigenvals corresponding eigenvectors\n",
    "\n",
    "$\\bf Quesetion :$ Why top k eigenvals corresponding eigenvectors can explain the most variance ?\n",
    "\n",
    "$\\bf Answer :$ Recall that Rayleigh quotient ensured that $\\displaystyle\n",
    "\\max_{\\bf v\\neq 0}\\frac{\\bf v^TCv}{\\bf v^Tv} = \\max_{\\bf \\left\\| v\\right\\| = 1}\\bf {v^TCv} = \\lambda_n\n",
    "$ where $\\lambda_n$ is the greatest eigenvalues and $\\bf v$ is the corresponding $\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $ eigenvectors, then $\\bf v^\\top Cvn = (Xv)^\\top (Xv)$ is the variance, and the greatest eigvalues corresponds eigenvectors attain the maximum.\n",
    "\n",
    "Finally, we project $X$ to the top k eigenvalues corresponding eigenvectors to obtain the new croodinates system, and these eigenvectors carry the most spatially information by using lower data memorey.\n",
    "\n",
    "## Algorithm \n",
    "\n",
    "$\\bf Input :$\n",
    "\n",
    "- `X` : An 2d array whose rows are samples and columns are features\n",
    "- `k` : An int object which represents the target dimension\n",
    "\n",
    "$\\bf Output :$\n",
    "\n",
    "- `Z` : An 2d array whose rows are samples and columns are features which have been reduced dimension\n",
    "\n",
    "$\\bf Step :$\n",
    "\n",
    "1. Normalized $X$ by `X - X.mean(axis = 0)`\n",
    "2. Calculate the covariance matrix $C = \\frac{1}{n}X^\\top X$\n",
    "3. perform EVD on $C$ by `np.linalg.eigh`\n",
    "4. Select top k eigenvals corresponding eigenvectors\n",
    "5. Let the eigenvectors corresponds to top k eigenvalues in order form a matrix  ùëà\n",
    "6. Return $Y = XU$\n",
    "\n",
    "## Code \n",
    "\n",
    "```python\n",
    "def pca(X, k):\n",
    "    '''\n",
    "    parameters\n",
    "    --------------------------------------------------------\n",
    "    X : An 2d array\n",
    "        which row are samples and columns are features\n",
    "    k : an int object\n",
    "        which represents the target dimension\n",
    "    '''\n",
    "    n_samples, n_features = X.shape\n",
    "    center = X.mean(axis = 0)\n",
    "    shifted_X = X - center\n",
    "    cov = shifted_X.T.dot(shifted_X) / n_samples\n",
    "    eigvals, eigvecs = np.linalg.eigh(cov)    \n",
    "    eigvecs = eigvecs[:,::-1]\n",
    "    eigvals = eigvals[::-1]\n",
    "    Y = X.dot(eigvecs[:,0:k])\n",
    "    explain_variance = eigvals\n",
    "    trace = np.trace(cov)\n",
    "    explain_variance_ratio = explain_variance / trace\n",
    "    \n",
    "    return Y, eigvecs[:,0:k], explain_variance, explain_variance_ratio, center, cov, shifted_X\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
